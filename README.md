# ğŸ§  Internal AI Assistant with Private Documents (RAG)

AI assistant designed to help teams quickly find information inside private documentation using Retrieval-Augmented Generation (RAG).

The system runs locally (offline-capable), preserves data privacy, and can be deployed as an internal service.

---

## ğŸš€ What problem does this solve?

Teams waste time:
- searching internal docs
- answering repetitive questions
- onboarding new members

This assistant:
- searches private documents semantically
- answers questions grounded only in internal data
- reduces time spent browsing documentation

---

## ğŸ—ï¸ Architecture Overview

User â†’ FastAPI API â†’ RAG Pipeline  
- Document Loader  
- Text Splitter  
- Embeddings  
- Vector Database  
- Local LLM  

---

## ğŸ§© Tech Stack

- **Python**
- **FastAPI** â€“ backend API
- **LangChain** â€“ RAG orchestration
- **FAISS** â€“ vector database
- **Ollama** â€“ local LLM runtime
  - LLM: LLaMA / Mistral
  - Embeddings: nomic-embed-text
- **Docker** â€“ containerized deployment

---

## ğŸ”’ Privacy First

- No external APIs required
- Documents never leave the local environment
- Suitable for internal company data

---

## ğŸ“ Project Structure

---

## âš™ï¸ How it works

1. Documents are loaded and split into chunks  
2. Chunks are converted into embeddings  
3. Embeddings are stored in a vector database  
4. User queries retrieve relevant chunks  
5. The LLM generates grounded answers  

---

## ğŸ“Œ Use cases

- Internal documentation assistant
- Knowledge base search
- Onboarding support
- Technical Q&A for teams

---

## ğŸ§ª Status

ğŸš§ Work in progress  
Next steps:
- API endpoints
- Docker setup
- Agentic extension (LangGraph)

---

## ğŸ‘¤ Author. Patricio Romero

Built by an AI Engineer focused on:
- GenAI systems
- Agentic workflows
- Production-ready AI solutions
